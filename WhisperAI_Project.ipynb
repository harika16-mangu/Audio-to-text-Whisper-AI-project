{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQVW759-XFxA",
        "outputId": "b53017a8-6e3a-4477-c51a-2d4d978ebc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-wx2c2big\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-wx2c2big\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=23e774316496b3c14fbaa3f5ccb742074eff43bcf310f0d88c28d172aa4622de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v6nb0why/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,002 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,239 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,789 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.0 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,273 kB]\n",
            "Fetched 24.3 MB in 2s (10.5 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installed Whisper AI. It's an open source speech recognization system by openai recognized to transcribe video and audio files across wide range of languages.Whisper AI has 4 different models to choose from namely tiny,base,small,medium,large and each has their own capabilities,\n",
        "and the models can be choosed based on our requirements namely english-only model and multilinguial model."
      ],
      "metadata": {
        "id": "OTbtG5zcYIVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/WIN_20230202_11_34_33_Pro.mp4\" --model medium.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA_YXnuUX2uN",
        "outputId": "ba0fe1d8-be94-4f2d-e7b0-c8c077fc8480"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:35<00:00, 43.1MiB/s]\n",
            "[00:00.000 --> 00:06.120]  Hi, I'm Harika. I'm a recent graduate of computer science engineering and my interest towards\n",
            "[00:06.120 --> 00:11.860]  statistics and data led me choose to pursue my career towards data analytics. And I completed\n",
            "[00:11.860 --> 00:17.800]  my internship in Salfa Cloud, where I implemented various concepts of cybersecurity, cloud computing,\n",
            "[00:17.800 --> 00:23.240]  and data pipelining. And I have an immense interest to learn more about machine learning,\n",
            "[00:23.240 --> 00:28.520]  natural language processing, data pipelining. So all that interest led me choose my career\n",
            "[00:28.520 --> 00:36.760]  towards data. And I'm, I'm a kind of person who always have to learn new skills, either\n",
            "[00:36.760 --> 00:42.960]  by myself or acquiring it from others. And I aspire to do whatever I need to in order\n",
            "[00:42.960 --> 00:49.320]  to fulfill my goals. I strategize my career path so that that path is more inclined towards\n",
            "[00:49.320 --> 00:55.240]  my strengths and interests. And apart from studies, I'm also good at playing badminton\n",
            "[00:55.240 --> 01:04.880]  and table tennis. So I think to start my career as a data analyst from such a reputed\n",
            "[01:04.880 --> 01:12.640]  company like Johnson & Johnson's will really help me to excel in my career. So I feel that\n",
            "[01:12.640 --> 01:22.120]  this opportunity for me is like a stepping stone of success to my career ahead. Thank\n",
            "[01:22.120 --> 01:22.320]  you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use this simple code whisper \"video file name\" and model model-type, this looks into the audio file and creates transcripts with high accuracy, i tried it by using medium model, the output is exactly as expected but 2 minor issues.\n",
        "1. It returned i have to learn new skills, but in audio i told i thrive to learn new skills\n",
        "2. My internship at Salfa cloud but in audio i told cipher cloud\n",
        "This transcripts is really very quick, accurate and reaches expectations.\n",
        "\n"
      ],
      "metadata": {
        "id": "MYhV9XJSbbQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpzMTkp6a_4d",
        "outputId": "0f601a7c-1093-4428-840f-c5620cee3880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "               [--output_dir OUTPUT_DIR]\n",
            "               [--output_format {txt,vtt,srt,tsv,json,all}]\n",
            "               [--verbose VERBOSE] [--task {transcribe,translate}]\n",
            "               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "               [--temperature TEMPERATURE] [--best_of BEST_OF]\n",
            "               [--beam_size BEAM_SIZE] [--patience PATIENCE]\n",
            "               [--length_penalty LENGTH_PENALTY]\n",
            "               [--suppress_tokens SUPPRESS_TOKENS]\n",
            "               [--initial_prompt INITIAL_PROMPT]\n",
            "               [--carry_initial_prompt CARRY_INITIAL_PROMPT]\n",
            "               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT]\n",
            "               [--fp16 FP16]\n",
            "               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "               [--logprob_threshold LOGPROB_THRESHOLD]\n",
            "               [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
            "               [--word_timestamps WORD_TIMESTAMPS]\n",
            "               [--prepend_punctuations PREPEND_PUNCTUATIONS]\n",
            "               [--append_punctuations APPEND_PUNCTUATIONS]\n",
            "               [--highlight_words HIGHLIGHT_WORDS]\n",
            "               [--max_line_width MAX_LINE_WIDTH]\n",
            "               [--max_line_count MAX_LINE_COUNT]\n",
            "               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n",
            "               [--clip_timestamps CLIP_TIMESTAMPS]\n",
            "               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n",
            "               audio [audio ...]\n",
            "\n",
            "positional arguments:\n",
            "  audio                 audio file(s) to transcribe\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         name of the Whisper model to use (default: turbo)\n",
            "  --model_dir MODEL_DIR\n",
            "                        the path to save model files; uses ~/.cache/whisper by\n",
            "                        default (default: None)\n",
            "  --device DEVICE       device to use for PyTorch inference (default: cuda)\n",
            "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
            "                        directory to save the outputs (default: .)\n",
            "  --output_format {txt,vtt,srt,tsv,json,all}, -f {txt,vtt,srt,tsv,json,all}\n",
            "                        format of the output file; if not specified, all\n",
            "                        available formats will be produced (default: all)\n",
            "  --verbose VERBOSE     whether to print out the progress and debug messages\n",
            "                        (default: True)\n",
            "  --task {transcribe,translate}\n",
            "                        whether to perform X->X speech recognition\n",
            "                        ('transcribe') or X->English translation ('translate')\n",
            "                        (default: transcribe)\n",
            "  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n",
            "                        language spoken in the audio, specify None to perform\n",
            "                        language detection (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "                        temperature to use for sampling (default: 0)\n",
            "  --best_of BEST_OF     number of candidates when sampling with non-zero\n",
            "                        temperature (default: 5)\n",
            "  --beam_size BEAM_SIZE\n",
            "                        number of beams in beam search, only applicable when\n",
            "                        temperature is zero (default: 5)\n",
            "  --patience PATIENCE   optional patience value to use in beam decoding, as in\n",
            "                        https://arxiv.org/abs/2204.05424, the default (1.0) is\n",
            "                        equivalent to conventional beam search (default: None)\n",
            "  --length_penalty LENGTH_PENALTY\n",
            "                        optional token length penalty coefficient (alpha) as\n",
            "                        in https://arxiv.org/abs/1609.08144, uses simple\n",
            "                        length normalization by default (default: None)\n",
            "  --suppress_tokens SUPPRESS_TOKENS\n",
            "                        comma-separated list of token ids to suppress during\n",
            "                        sampling; '-1' will suppress most special characters\n",
            "                        except common punctuations (default: -1)\n",
            "  --initial_prompt INITIAL_PROMPT\n",
            "                        optional text to provide as a prompt for the first\n",
            "                        window. (default: None)\n",
            "  --carry_initial_prompt CARRY_INITIAL_PROMPT\n",
            "                        if True, prepend initial_prompt to every internal\n",
            "                        decode() call. May reduce the effectiveness of\n",
            "                        condition_on_previous_text (default: False)\n",
            "  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n",
            "                        if True, provide the previous output of the model as a\n",
            "                        prompt for the next window; disabling may make the\n",
            "                        text inconsistent across windows, but the model\n",
            "                        becomes less prone to getting stuck in a failure loop\n",
            "                        (default: True)\n",
            "  --fp16 FP16           whether to perform inference in fp16; True by default\n",
            "                        (default: True)\n",
            "  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n",
            "                        temperature to increase when falling back when the\n",
            "                        decoding fails to meet either of the thresholds below\n",
            "                        (default: 0.2)\n",
            "  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n",
            "                        if the gzip compression ratio is higher than this\n",
            "                        value, treat the decoding as failed (default: 2.4)\n",
            "  --logprob_threshold LOGPROB_THRESHOLD\n",
            "                        if the average log probability is lower than this\n",
            "                        value, treat the decoding as failed (default: -1.0)\n",
            "  --no_speech_threshold NO_SPEECH_THRESHOLD\n",
            "                        if the probability of the <|nospeech|> token is higher\n",
            "                        than this value AND the decoding has failed due to\n",
            "                        `logprob_threshold`, consider the segment as silence\n",
            "                        (default: 0.6)\n",
            "  --word_timestamps WORD_TIMESTAMPS\n",
            "                        (experimental) extract word-level timestamps and\n",
            "                        refine the results based on them (default: False)\n",
            "  --prepend_punctuations PREPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation\n",
            "                        symbols with the next word (default: \"'“¿([{-)\n",
            "  --append_punctuations APPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation\n",
            "                        symbols with the previous word (default:\n",
            "                        \"'.。,，!！?？:：”)]}、)\n",
            "  --highlight_words HIGHLIGHT_WORDS\n",
            "                        (requires --word_timestamps True) underline each word\n",
            "                        as it is spoken in srt and vtt (default: False)\n",
            "  --max_line_width MAX_LINE_WIDTH\n",
            "                        (requires --word_timestamps True) the maximum number\n",
            "                        of characters in a line before breaking the line\n",
            "                        (default: None)\n",
            "  --max_line_count MAX_LINE_COUNT\n",
            "                        (requires --word_timestamps True) the maximum number\n",
            "                        of lines in a segment (default: None)\n",
            "  --max_words_per_line MAX_WORDS_PER_LINE\n",
            "                        (requires --word_timestamps True, no effect with\n",
            "                        --max_line_width) the maximum number of words in a\n",
            "                        segment (default: None)\n",
            "  --threads THREADS     number of threads used by torch for CPU inference;\n",
            "                        supercedes MKL_NUM_THREADS/OMP_NUM_THREADS (default:\n",
            "                        0)\n",
            "  --clip_timestamps CLIP_TIMESTAMPS\n",
            "                        comma-separated list start,end,start,end,...\n",
            "                        timestamps (in seconds) of clips to process, where the\n",
            "                        last end timestamp defaults to the end of the file\n",
            "                        (default: 0)\n",
            "  --hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD\n",
            "                        (requires --word_timestamps True) skip silent periods\n",
            "                        longer than this threshold (in seconds) when a\n",
            "                        possible hallucination is detected (default: None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper -h returns all the list of services available in this open source speech recognization model by openai."
      ],
      "metadata": {
        "id": "rypHQ71DcW_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/WIN_20230202_11_34_33_Pro.mp4\" --task translate --language hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESAA40Z4dBV5",
        "outputId": "d935f210-56ed-47d2-e52f-afe8549628a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "             ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py\", line 595, in cli\n",
            "    model = load_model(model_name, device=device, download_root=model_dir)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/whisper/__init__.py\", line 161, in load_model\n",
            "    return model.to(device)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1369, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 928, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 928, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 928, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 2 more times]\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 955, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1355, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 2075 has 10.13 GiB memory in use. Process 46423 has 4.59 GiB memory in use. Of the allocated memory 4.29 GiB is allocated by PyTorch, and 192.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,things are getting more intresting, i'm translating my audio file into hindi language,its didn't successfully translate well, the output is combination os hindi and english"
      ],
      "metadata": {
        "id": "_pq31kr2du5L"
      }
    }
  ]
}